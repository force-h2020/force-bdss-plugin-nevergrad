Introduction
------------

.. contents:: Table of contents


******************************
The ``force-nevergrad`` plugin
******************************

The `force-bdss-plugin-nevergrad  <https://github.com/force-h2020/force-bdss-plugin-nevergrad>`_ contributes a wrapper
around the gradient-free optimization library `nevergrad <https://github.com/facebookresearch/nevergrad>`_.
The wrapper consists of two parts:

#. The optimization engine ``NevergradOptimizerEngine``, which is used as a backend tool
   for worklow optimization.
   The engine inherits from the ``BaseOptimizerEngine`` class.
   It provides a public ``optimize()`` method that  yields workflow input data and output results, and any additional
   metadata generated by the engine.
#. The basic multi-criteria optimizer ``NevergradMCO(BaseMCO)``. The ``MCO`` class configures the ``NevergradOptimizerEngine``
   instance, and executes the optimizer.
   The standard progress events ``MCOProgressEvent`` are generated during the optimization, containing the optimal points and
   optimal KPIs.

Users can perform the gradient-free optimization of their workflows by specifying the ``nevergrad_mco`` as the MCO model id:

.. code-block:: json

    {
        "workflow": {
            "mco_model": {
                "id": "force.bdss.nevergrad.plugin.wrapper.v0.factory.nevergrad_mco",
                "model_data": {
                    "algorithms": "OnePlusOne",
                    "budget": 200,
                    "verbose_run": true
                }
            }
        }
    }

Users can import the ``NevergradOptimizerEngine`` and implement their own family of ``MCOFactory, MCOModel, MCO`` classes
that uses the ``nevergrad`` as the optimizer engine.


Nevergrad optimizer engine
################################


The ``NevergradOptimizerEngine`` inherits from the ``BaseOptimizerEngine``, and requires a list of MCO parameters and KPIs
to be specified upon the engine instantiation.
The ``NevergradOptimizerEngine`` parses the standard ``force-bdss`` MCO parameters into its own internal types.
The ``NevergradOptimizerEngine`` can handle

* continuous numerical parameters (these implement ``lower_bound`` and ``upper_bound`` attributes),
* ordered discrete parameters (these implement ``levels`` attribute),
* unordered discrete parameters (these implement ``categories`` attribute), and
* constant-valued paramaters (these implement ``value`` attribute).

The KPIs are implicitly assumed to be minimized over the course of optimization.
User should provide the upper bounds for the KPIs, which is required by the multiobjective optimization algorithm,
implemented in ``nevergrad``.
Due to the backwards compatibility issues, the KPIs' upper bounds must be specified by the ``scale_factor`` field.
(This will likely be fixed in the next release, see `this issue <https://github.com/force-h2020/force-bdss/issues/293>`_).

The choice of possible optimization algorithms is provided by the ``nevergrad`` library itself.
If users would like to implement their own optimization algorithm,
`this reference <https://github.com/facebookresearch/nevergrad/blob/master/docs/contributing.rst#adding-an-algorithm>`_
explains general guidelines how to do that.
Please be aware that the optimization convergence, efficiency, and computation time strongly depends on the optimization strategy,
and it might require some effort to find an algorithm suitable for a particular problem.


Nevergrad MCO
################################

The ``NevergradMCO`` and ``NevergradMCOModel`` implement the minimal working example of an MCO.
Users are strongly encouraged to develop or extend the existing methods.

The ``budget`` attribute defines the allowed number of objective function calls.

It is possible to choose between a ``verbose_run = True`` and ``verbose_run = False`` options for the MCO model.

The verbose run will notify the Workflow Manager every time a point is evaluated in the parameter space, and the user will
observe the number of data entries equal to the optimization budget.
This also means that the Pareto front is not generated during the optimization run, since the Pareto front can only be
generated when all the data entries are available.

Setting the ``verbose_run`` to ``False`` will result in no data entries exposed to the user during the optimization.
When the optimization budget is finally spent by the optimizer, the ``NevergradOptimizerEngine`` will identify the Pareto front,
and notify the user with the Pareto-optimal data entries.


*******************************
``nevergrad`` basics and how-to
*******************************

Please refer to the `official documentation <https://github.com/facebookresearch/nevergrad/tree/master/docs>`_ first:
the API, design principles, and guidelines can (and almost certainly will) change.

6 things to know about ``nevergrad``
####################################

* ``nevergrad`` is a great tool for gradient-free optimization of generic problems, with the possibility to optimize numerical
  and categorical values. Users have access to benchmarking tools and many algorithms out of the box.
* ``nevergrad`` is great for single objective optimization with “easy” constraints: when the parameter space is assumed to
  violate the constraints on a small part of the search space. This means: no equality constraints, no severe inequality
  constraints. Of course, users can implement the constraints handling ourselves.
* ``nevergrad`` can be used for multiobjective optimization: either by explicit conversion of the MCO problem into a single
  objective problem (for example, using a weighted approach), or using the neveregrad hypervolume approach. The later
  one provides automated Pareto-front calculation.
* It is relatively easy to have custom search spaces: from standard bounded parameters and categorical values,
  to variables with logarithmic distribution (useful for ML applications), and user-defined distributions.
* Ask-and-tell interface allows us to
    * Yield the optimization results at runtime when a new input point is explored, and process them if we want to, and
    * Instead of using the internal ``nevergrad``’s recommendation system for search space exploration, we can
      choose what combinations of parameters to explore, and Nevergrad will infer from that.
* ``nevergrad`` can perform parallel optimization usign ``multiprocessing`` and GPUs.


Ask, tell, and provide recommendation
#####################################

``nevergrad`` implements the ``ask and tell`` interface.
The three key methods for this interface are:

* ``ask``: suggest a candidate on which to evaluate the function to optimize.
* ``tell``: for updated the optimizer with the value of the function for a candidate.
* ``provide_recommendation``: returns the candidate the algorithms considers the best.

A toy example shows the usage of these methods:

.. code-block:: python

    import nevergrad as ng

    def square(x, y=12):
        """
        Convex objective function
        """
        return sum((x - 1.5) ** 2) + abs(y)

    params = ng.p.Instrumentation(ng.p.Array(shape=(2,)), y=ng.p.Scalar())
    optimizer = ng.optimizers.OnePlusOne(parametrization=params, budget=100)

    for _ in range(optimizer.budget):
        x = optimizer.ask()
        value = square(*x.args, **x.kwargs)
        optimizer.tell(x, value)


    recommendation = optimizer.provide_recommendation()
    print(recommendation.value)

Advanced users can implement their own ``NevergradOptimizerEngine.optimize()`` method, that asks the
optimization algorithm for a point to evaluate via ``ask``, submits multiple objective evaluation calls to separate
processes, and then notifies the via optimization algorithm about the results via ``tell``.
The optimization algorithm can then provide a recommendation on what is considered to be an "optimal" input for this
objective.

Since the ``NevergradOptimizerEngine`` is separated from the ``MCO.run()``, the ``MCO`` implementation is independent of
how the gradient-free algorithm is performing the optimization under the hood.
